{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb63dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "847c7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "If you can't answer the question based on the context, respond with \"mujhe nahi pata\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# Initialize LLM and Parser\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f207ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and splitting the PDF...\n",
      "PDF split into 8 chunks.\n",
      "Creating embeddings and storing them in ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/63sb0y8s1c976b6hx4kv1zch0000gn/T/ipykernel_90134/861891986.py:20: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
      "2025-08-05 23:51:14.311 INFO    chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF processed and retriever is ready!\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTANT ---\n",
    "# Change this path to the location of your PDF file\n",
    "pdf_path = \"/Users/pulkitchauhan/coding/ollam/1723042610822.pdf\" \n",
    "# For example: pdf_path = \"C:/Users/YourUser/Documents/report.pdf\" or pdf_path = \"./data/my_paper.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Error: The file '{pdf_path}' was not found.\")\n",
    "    print(\"Please update the 'pdf_path' variable with the correct path to your PDF.\")\n",
    "else:\n",
    "    # 1. Load and split the PDF\n",
    "    print(\"Loading and splitting the PDF...\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    split_docs = splitter.split_documents(pages)\n",
    "    print(f\"PDF split into {len(split_docs)} chunks.\")\n",
    "\n",
    "    # 2. Create embeddings and store in a Chroma vector store\n",
    "    print(\"Creating embeddings and storing them in ChromaDB...\")\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"pdf_qa_collection\"  # A unique name for the collection\n",
    "    )\n",
    "    \n",
    "    # 3. Create the retriever\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    print(\"âœ… PDF processed and retriever is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43cc521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving relevant documents for the question: 'uski kitni girlfriend hai?'\n",
      "Invoking the LLM to generate an answer...\n",
      "\n",
      "==================================================\n",
      "ðŸ¤– Answer:\n",
      "==================================================\n",
      "Mujhe nahi pata. (I don't know) Shivam Shrivastava ki profile mein koi information nahin hai jo uske relationship status ko show karti ho.\n"
     ]
    }
   ],
   "source": [
    "# --- Ask your question here ---\n",
    "question = \"uski kitni girlfriend hai?\"\n",
    "# -----------------------------\n",
    "\n",
    "if 'retriever' in locals():\n",
    "    # 1. Retrieve relevant documents based on the question\n",
    "    print(f\"Retrieving relevant documents for the question: '{question}'\")\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # 2. Combine the content of the retrieved documents into a single context string\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # 3. Invoke the chain with the context and question\n",
    "    print(\"Invoking the LLM to generate an answer...\")\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    # 4. Print the final answer\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ¤– Answer:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"\\nRetriever not initialized. Please run the PDF processing cell (In[4]) successfully first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a07861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
